{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pylab as pl\n",
    "\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Learning Generative Models with Sinkhorn Divergences](https://arxiv.org/abs/1706.00292)\n",
    "### by Aude Genevay, Gabriel Peyr√©, and Marco Cuturi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper proposes the *Sinkhorn divergence*, an optimal transport (OT) based optimization objective amenable to auto-differentiation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.  Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook, we'll use data drawn from a Gaussian Mixture Model (GMM).  Here is a function to draw samples from a GMM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw sample from mixture model\n",
    "# k ~ Mult(pi)\n",
    "# z ~ f_k\n",
    "def draw_samples(pi_arr, mu_arr, cov_arr, n_samples=100):\n",
    "    comp_arr = np.random.multinomial(n_samples, pi_arr)\n",
    "    z = []\n",
    "    y = []\n",
    "    for idx, count in enumerate(comp_arr):\n",
    "        for c in xrange(count):\n",
    "            y.append(idx)\n",
    "            z.append(np.random.multivariate_normal(mu_s[idx], cov_s[idx]))\n",
    "    return np.array(z), np.array(y)[np.newaxis].T\n",
    "\n",
    "def shuffle_in_unison_inplace(a, b):\n",
    "    assert a.shape[0] == b.shape[0]\n",
    "    p = np.random.permutation(a.shape[0])\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 300 # number of datapoints \n",
    "input_d = 2\n",
    "\n",
    "# Define mixture model\n",
    "pi = np.array([.35, .65])\n",
    "mu_s = [np.array([-5., -5.]), np.array([5., 5.])]\n",
    "cov_s = [np.array([[1., 0.], [0., 1.]]), np.array([[1., 0.], [0., 1.]])]\n",
    "\n",
    "# draw_samples\n",
    "X_train, y_train = draw_samples(pi, mu_s, cov_s, N)\n",
    "\n",
    "# shuffle \n",
    "X_train, y_train = shuffle_in_unison_inplace(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(X, Y):\n",
    "    return tf.reduce_sum((X - Y)**2, keep_dims=True)\n",
    "\n",
    "def sinkhorn_loss(X, Y, epsilon=.05, L=3, n=10):\n",
    "    c = np.zeros((n, n)).tolist()\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            c[i][j] = cost(X[i, :], Y[j, :]) \n",
    "    \n",
    "    c = tf.transpose(tf.concat(1, [tf.expand_dims(tf.concat(0, t), 1) for t in c]))\n",
    "    K = tf.exp(-c/epsilon) \n",
    "    \n",
    "    a = tf.ones((n, 1))\n",
    "    b = tf.ones((n, 1))\n",
    "    for l in range(L):\n",
    "        a = 1./(tf.matmul(K, b) + 1e-8)\n",
    "        b = 1./(tf.matmul(tf.transpose(K), a) + 1e-8)\n",
    "    \n",
    "    return tf.matmul(tf.transpose(tf.matmul(tf.mul(K, c), b)), a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a neural network instead of a regression model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_neural_net(layer_sizes, std=.001):\n",
    "    # layer_sizes is a list of the input size, hidden layer sizes, and output size\n",
    "    params = {'w':[], 'b':[]}\n",
    "    for n_in, n_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "        params['w'].append(tf.Variable(tf.random_normal([n_in, n_out], stddev=std)))\n",
    "        params['b'].append(tf.Variable(tf.zeros([n_out,])))\n",
    "    return params\n",
    "\n",
    "def neural_net(X, params):\n",
    "    h = [X]\n",
    "    for w,b in zip(params['w'][:-1], params['b'][:-1]):\n",
    "        h.append( tf.nn.relu( tf.matmul(h[-1], w) + b ) )\n",
    "    # NOTE: no output activation.  TF will take care of this in pre-defined loss functions\n",
    "    return tf.matmul(h[-1], params['w'][-1]) + params['b'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the network's symbolic output and cost like we did for regression before..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Generative Adversarial Network (GAN)\n",
    "Now let's explore [Generative Adversarial Networks (GANs)](https://arxiv.org/abs/1406.2661) with TensorFlow.  GANs are composed of two neural networks.  One network is trying to classify simulated data from the real data.  The other network is trying to simulate data in such a way that the first net will be fooled.  The result of this process is that the second net gets better and better at simulating realistic data until eventually that data is indistinguishable from the real data.  The computational pipeline is summarized in the diagram below: \n",
    "![GAN_pipeline](./graphics/GAN.png)  GANs are refered to as 'implicit generative models' as there is an implied likelihood but not a well-specified one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the symbolic variables again.  This time we need another one, Z, that will be the samples drawn from the generator's latent space..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent_d = 50 # z ~ p(z), GAN prior\n",
    "hidden_d = 500 # num. of hidden units in NN\n",
    "\n",
    "### Make symbolic variables\n",
    "X = tf.placeholder(\"float\", [None, input_d]) # samples to discriminate\n",
    "Z = tf.placeholder(\"float\", [None, latent_d]) # samples from generator's latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the discriminator model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the generator model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator_params = init_neural_net([latent_d, hidden_d, input_d])\n",
    "generator_out = neural_net(Z, generator_params)\n",
    "\n",
    "loss = tf.reduce_sum(2*sinkhorn_loss(X, generator_out) - sinkhorn_loss(X, X) - sinkhorn_loss(generator_out, generator_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x164887210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8b2aa7de4fc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# perform update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprior_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;31m#print l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m#print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def plot_densities(true_data, samples):\n",
    "    # clear the plot\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    plt.scatter(true_data[:,0], true_data[:,1], s=100, color='k', marker='x', label=\"data\")\n",
    "    plt.scatter(samples[:,0], samples[:,1], s=100, color='r', marker='o', label=\"samples\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.xlim([-15,15])\n",
    "    plt.ylim([-15,15])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Set training params\n",
    "n_epochs = 150\n",
    "learning_rate = 0.001\n",
    "\n",
    "# create training ops\n",
    "train_generator = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=generator_params['w']+generator_params['b'])\n",
    "\n",
    "generator_weights = None\n",
    "generator_biases = None\n",
    "with tf.Session() as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for epoch_idx in xrange(n_epochs):\n",
    "        \n",
    "        loss_tracker = 0.\n",
    "        \n",
    "        # train generator\n",
    "        for idx in xrange(N/10):\n",
    "            # sample from generator again\n",
    "            prior_samples = np.random.normal(size=(10, latent_d))\n",
    "        \n",
    "            # perform update\n",
    "            _, l = session.run([train_generator, loss], feed_dict={Z: prior_samples, X: X_train[idx*10:(idx+1)*10, :]})\n",
    "            #print l\n",
    "            #print\n",
    "            loss_tracker += l\n",
    "        \n",
    "        # visualize progress\n",
    "        if epoch_idx%5 == 0: \n",
    "            plot_densities(X_train, session.run(generator_out, feed_dict={Z: np.random.normal(size=(N/2, latent_d))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GAN can capture one mode well but not both (at least for the settings I've tried).  This is known problem: \"A common problem with GAN framework is that the generator tends to only generate samples that are clustered in one or a few modes of the regions of high data density, instead of spanning the whole range\" [[source]](https://arxiv.org/pdf/1609.03126v2.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Training a GAN on MNIST\n",
    "Let's try to train a GAN on a subset of MNIST..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "# reduce dataset and normalize to [0,1]\n",
    "random_idxs = range(mnist.data.shape[0])\n",
    "shuffle(random_idxs)\n",
    "mnist_images = mnist.data[random_idxs[:5000],:] / 255.\n",
    "\n",
    "# show the first image\n",
    "plt.imshow(np.reshape(mnist_images[0,:] * 255., (28, 28)), cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same symbolic variables, discriminator, and generator as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, input_d = mnist_images.shape\n",
    "latent_d = 100 # z ~ p(z), GAN prior\n",
    "hidden_d_discrim = 1000 # num. of hidden units in discrim NN\n",
    "hidden_d_gen = 500 # num. of hidden units in gen NN\n",
    "\n",
    "### Make symbolic variables\n",
    "X = tf.placeholder(\"float\", [None, input_d]) # samples to discriminate\n",
    "Z = tf.placeholder(\"float\", [None, latent_d]) # samples to discriminate\n",
    "Y = tf.placeholder(\"float\", [None, 1]) # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_params = init_neural_net([input_d, hidden_d_discrim, hidden_d_discrim, 1]) \n",
    "discrim_out = neural_net(X, discrim_params)\n",
    "discrim_cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(discrim_out, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator_params = init_neural_net([latent_d, hidden_d_gen, input_d])\n",
    "generator_out = neural_net(Z, generator_params)\n",
    "\n",
    "# This line is new.  The images are on [0,1] so we need to apply a sigmoid to the samples.\n",
    "generator_out_squashed = tf.nn.sigmoid(generator_out)\n",
    "\n",
    "discrim_out_genUpdate = neural_net(generator_out_squashed, discrim_params)\n",
    "generator_cost = tf.reduce_mean(-tf.nn.sigmoid_cross_entropy_with_logits(discrim_out_genUpdate, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the GAN..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make labels for training\n",
    "X_true = mnist_images\n",
    "Y_true = np.ones((N,1))\n",
    "Y_gen = np.zeros((N,1))\n",
    "Y_concat = np.vstack([Y_true, Y_gen])\n",
    "\n",
    "# Set training params\n",
    "n_epochs = 25\n",
    "n_discrim_updates = 1\n",
    "n_generator_updates = 1\n",
    "d_learning_rate = .0002\n",
    "g_learning_rate = .00005\n",
    "batch_size = 120\n",
    "n_batches = N/batch_size\n",
    "\n",
    "# create training ops\n",
    "train_discriminator = tf.train.AdamOptimizer(d_learning_rate).minimize(discrim_cost, var_list=discrim_params['w']+discrim_params['b'])\n",
    "train_generator = tf.train.AdamOptimizer(g_learning_rate).minimize(generator_cost, var_list=generator_params['w']+generator_params['b'])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for epoch_idx in xrange(n_epochs):\n",
    "        \n",
    "        # train discriminator\n",
    "        discrim_error = 0.\n",
    "        for idx in xrange(n_discrim_updates):\n",
    "            # sample from generator\n",
    "            prior_samples = np.random.normal(size=(N, latent_d))\n",
    "            genSamples = session.run(generator_out, feed_dict={Z: prior_samples})\n",
    "        \n",
    "            # make dataset and shuffle\n",
    "            train_X = np.vstack([X_true, genSamples])\n",
    "            train_X, train_Y = shuffle_in_unison_inplace(train_X, Y_concat)\n",
    "        \n",
    "            # perform batch updates\n",
    "            epoch_discrim_error = 0.\n",
    "            for batch_idx in xrange(n_batches):\n",
    "                _, l = session.run([train_discriminator, discrim_cost], \\\n",
    "                                   feed_dict={X: train_X[batch_idx*batch_size:(batch_idx+1)*batch_size], \\\n",
    "                                              Y: train_Y[batch_idx*batch_size:(batch_idx+1)*batch_size]})\n",
    "                epoch_discrim_error += l\n",
    "            discrim_error += epoch_discrim_error/n_batches\n",
    "            \n",
    "        # print \"Epoch %d.  Discriminator error: %.3f\" %(epoch_idx, discrim_error)\n",
    "        \n",
    "        # train generator\n",
    "        for idx in xrange(n_generator_updates):\n",
    "            # sample from generator again\n",
    "            prior_samples = np.random.normal(size=(N, latent_d))\n",
    "        \n",
    "            # perform batch updates\n",
    "            for batch_idx in xrange(n_batches):\n",
    "                session.run(train_generator, feed_dict={Z: prior_samples[batch_idx*batch_size:(batch_idx+1)*batch_size], \\\n",
    "                                                        Y: Y_gen[batch_idx*batch_size:(batch_idx+1)*batch_size]})\n",
    "        \n",
    "        # visualize a sample to gauge progress\n",
    "        mnist_sample = session.run(generator_out_squashed, feed_dict={Z:np.random.normal(size=(1, latent_d))})\n",
    "        display.clear_output(wait=True)\n",
    "        plt.imshow(np.reshape(mnist_sample * 255., (28, 28)), cmap='Greys_r')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There has been recent work on trying to understand what GANs are doing in terms of classic estimation principles.  See [Shakir Mohamed's note](https://arxiv.org/abs/1610.03483) characterizing GANs as performing ratio tests, $p(x)/q(x)$ where $p(x)$ is the true distribution and $q(x)$ is the simulated one.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
