{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pylab as pl\n",
    "\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Learning Generative Models with Sinkhorn Divergences](https://arxiv.org/abs/1706.00292)\n",
    "### by Aude Genevay, Gabriel Peyr√©, and Marco Cuturi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper proposes the *Sinkhorn divergence*, an optimal transport (OT) based optimization objective amenable to auto-differentiation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.  Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook, we'll use data drawn from a Gaussian Mixture Model (GMM).  Here is a function to draw samples from a GMM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw sample from mixture model\n",
    "# k ~ Mult(pi)\n",
    "# z ~ f_k\n",
    "def draw_samples(pi_arr, mu_arr, cov_arr, n_samples=100):\n",
    "    comp_arr = np.random.multinomial(n_samples, pi_arr)\n",
    "    z = []\n",
    "    y = []\n",
    "    for idx, count in enumerate(comp_arr):\n",
    "        for c in xrange(count):\n",
    "            y.append(idx)\n",
    "            z.append(np.random.multivariate_normal(mu_s[idx], cov_s[idx]))\n",
    "    return np.array(z), np.array(y)[np.newaxis].T\n",
    "\n",
    "def shuffle_in_unison_inplace(a, b):\n",
    "    assert a.shape[0] == b.shape[0]\n",
    "    p = np.random.permutation(a.shape[0])\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 500 # number of datapoints \n",
    "input_d = 2\n",
    "\n",
    "# Define mixture model\n",
    "pi = np.array([.05, .95])\n",
    "mu_s = [np.array([-5., -5.]), np.array([5., 5.])]\n",
    "cov_s = [np.array([[1., 0.], [0., 1.]]), np.array([[1., 0.], [0., 1.]])]\n",
    "\n",
    "# draw_samples\n",
    "X_train, y_train = draw_samples(pi, mu_s, cov_s, N)\n",
    "\n",
    "# shuffle \n",
    "X_train, y_train = shuffle_in_unison_inplace(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_neural_net(layer_sizes, std=.001):\n",
    "    # layer_sizes is a list of the input size, hidden layer sizes, and output size\n",
    "    params = {'w':[], 'b':[]}\n",
    "    for n_in, n_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "        params['w'].append(tf.Variable(tf.random_normal([n_in, n_out], stddev=std)))\n",
    "        params['b'].append(tf.Variable(tf.zeros([n_out,])))\n",
    "    return params\n",
    "\n",
    "def neural_net(X, params):\n",
    "    h = [X]\n",
    "    for w,b in zip(params['w'][:-1], params['b'][:-1]):\n",
    "        h.append( tf.nn.relu( tf.matmul(h[-1], w) + b ) )\n",
    "    # NOTE: no output activation.  TF will take care of this in pre-defined loss functions\n",
    "    return tf.matmul(h[-1], params['w'][-1]) + params['b'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a neural network instead of a regression model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the network's symbolic output and cost like we did for regression before..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Generative Adversarial Network (GAN)\n",
    "Now let's explore [Generative Adversarial Networks (GANs)](https://arxiv.org/abs/1406.2661) with TensorFlow.  GANs are composed of two neural networks.  One network is trying to classify simulated data from the real data.  The other network is trying to simulate data in such a way that the first net will be fooled.  The result of this process is that the second net gets better and better at simulating realistic data until eventually that data is indistinguishable from the real data.  The computational pipeline is summarized in the diagram below: \n",
    "![GAN_pipeline](./graphics/GAN.png)  GANs are refered to as 'implicit generative models' as there is an implied likelihood but not a well-specified one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the symbolic variables again.  This time we need another one, Z, that will be the samples drawn from the generator's latent space..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "latent_d = 5 # z ~ p(z), GAN prior\n",
    "hidden_d = 20 # num. of hidden units in NN\n",
    "\n",
    "### Make symbolic variables\n",
    "X = tf.placeholder(\"float\", [None, input_d]) # samples to discriminate\n",
    "Z = tf.placeholder(\"float\", [None, latent_d]) # samples from generator's latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def cost(X, Y):\n",
    "#    return tf.reduce_sum((X - Y)**2, keep_dims=True)\n",
    "\n",
    "def sinkhorn_loss(X, Y, epsilon=.5, L=10):\n",
    "    \n",
    "    X_new = tf.expand_dims(X, 1)\n",
    "    Y_new = tf.expand_dims(Y, 0)\n",
    "    \n",
    "    diff = X_new - Y_new\n",
    "    c = tf.reduce_sum(diff**2, reduction_indices=-1)\n",
    "    K = tf.exp(-c/epsilon) \n",
    "    \n",
    "    a = tf.ones((tf.shape(Y)[0], 1))\n",
    "    b = tf.ones((tf.shape(Y)[0], 1))\n",
    "    for l in range(L):\n",
    "        a = 1./(tf.matmul(K, b) + 1e-12)\n",
    "        b = 1./(tf.matmul(tf.transpose(K), a) + 1e-12)\n",
    "    \n",
    "    return tf.matmul(tf.transpose(tf.matmul(tf.mul(K, c), b)), a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the discriminator model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the generator model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator_params = init_neural_net([latent_d, hidden_d, input_d])\n",
    "generator_out = neural_net(Z, generator_params)\n",
    "\n",
    "loss = tf.reduce_sum(2.*sinkhorn_loss(X, generator_out) - sinkhorn_loss(X, X) - sinkhorn_loss(generator_out, generator_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGPhEREWCgU9EVCQY+EREReL/A7OzAcSll1XDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1219288d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_densities(true_data, samples):\n",
    "    # clear the plot\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    plt.scatter(true_data[:,0], true_data[:,1], s=100, color='k', marker='x', label=\"data\")\n",
    "    plt.scatter(samples[:,0], samples[:,1], s=100, color='r', marker='o', label=\"samples\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.xlim([-15,15])\n",
    "    plt.ylim([-15,15])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Set training params\n",
    "n_epochs = 350\n",
    "learning_rate = 0.001\n",
    "\n",
    "# create training ops\n",
    "train_generator = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=generator_params['w']+generator_params['b'])\n",
    "\n",
    "generator_weights = None\n",
    "generator_biases = None\n",
    "with tf.Session() as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for epoch_idx in xrange(n_epochs):\n",
    "        \n",
    "        loss_tracker = 0.\n",
    "        \n",
    "        # train generator\n",
    "        for idx in xrange(N/batch_size):\n",
    "            # sample from generator again\n",
    "            prior_samples = np.random.normal(size=(batch_size, latent_d))\n",
    "        \n",
    "            # perform update\n",
    "            _, l = session.run([train_generator, loss], feed_dict={Z: prior_samples, X: X_train[idx*batch_size:(idx+1)*batch_size, :]})\n",
    "            #print l\n",
    "            #print\n",
    "            loss_tracker += l\n",
    "        \n",
    "        # visualize progress\n",
    "        if epoch_idx%5 == 0: \n",
    "            plot_densities(X_train, session.run(generator_out, feed_dict={Z: np.random.normal(size=(N/2, latent_d))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GAN can capture one mode well but not both (at least for the settings I've tried).  This is known problem: \"A common problem with GAN framework is that the generator tends to only generate samples that are clustered in one or a few modes of the regions of high data density, instead of spanning the whole range\" [[source]](https://arxiv.org/pdf/1609.03126v2.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Training a GAN on MNIST\n",
    "Let's try to train a GAN on a subset of MNIST..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "# reduce dataset and normalize to [0,1]\n",
    "random_idxs = range(mnist.data.shape[0])\n",
    "shuffle(random_idxs)\n",
    "mnist_images = mnist.data[random_idxs[:5000],:] / 255.\n",
    "\n",
    "# show the first image\n",
    "plt.imshow(np.reshape(mnist_images[0,:] * 255., (28, 28)), cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same symbolic variables, discriminator, and generator as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, input_d = mnist_images.shape\n",
    "latent_d = 100 # z ~ p(z), GAN prior\n",
    "hidden_d_discrim = 1000 # num. of hidden units in discrim NN\n",
    "hidden_d_gen = 500 # num. of hidden units in gen NN\n",
    "\n",
    "### Make symbolic variables\n",
    "X = tf.placeholder(\"float\", [None, input_d]) # samples to discriminate\n",
    "Z = tf.placeholder(\"float\", [None, latent_d]) # samples to discriminate\n",
    "Y = tf.placeholder(\"float\", [None, 1]) # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_params = init_neural_net([input_d, hidden_d_discrim, hidden_d_discrim, 1]) \n",
    "discrim_out = neural_net(X, discrim_params)\n",
    "discrim_cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(discrim_out, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator_params = init_neural_net([latent_d, hidden_d_gen, input_d])\n",
    "generator_out = neural_net(Z, generator_params)\n",
    "\n",
    "# This line is new.  The images are on [0,1] so we need to apply a sigmoid to the samples.\n",
    "generator_out_squashed = tf.nn.sigmoid(generator_out)\n",
    "\n",
    "discrim_out_genUpdate = neural_net(generator_out_squashed, discrim_params)\n",
    "generator_cost = tf.reduce_mean(-tf.nn.sigmoid_cross_entropy_with_logits(discrim_out_genUpdate, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the GAN..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make labels for training\n",
    "X_true = mnist_images\n",
    "Y_true = np.ones((N,1))\n",
    "Y_gen = np.zeros((N,1))\n",
    "Y_concat = np.vstack([Y_true, Y_gen])\n",
    "\n",
    "# Set training params\n",
    "n_epochs = 25\n",
    "n_discrim_updates = 1\n",
    "n_generator_updates = 1\n",
    "d_learning_rate = .0002\n",
    "g_learning_rate = .00005\n",
    "batch_size = 120\n",
    "n_batches = N/batch_size\n",
    "\n",
    "# create training ops\n",
    "train_discriminator = tf.train.AdamOptimizer(d_learning_rate).minimize(discrim_cost, var_list=discrim_params['w']+discrim_params['b'])\n",
    "train_generator = tf.train.AdamOptimizer(g_learning_rate).minimize(generator_cost, var_list=generator_params['w']+generator_params['b'])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for epoch_idx in xrange(n_epochs):\n",
    "        \n",
    "        # train discriminator\n",
    "        discrim_error = 0.\n",
    "        for idx in xrange(n_discrim_updates):\n",
    "            # sample from generator\n",
    "            prior_samples = np.random.normal(size=(N, latent_d))\n",
    "            genSamples = session.run(generator_out, feed_dict={Z: prior_samples})\n",
    "        \n",
    "            # make dataset and shuffle\n",
    "            train_X = np.vstack([X_true, genSamples])\n",
    "            train_X, train_Y = shuffle_in_unison_inplace(train_X, Y_concat)\n",
    "        \n",
    "            # perform batch updates\n",
    "            epoch_discrim_error = 0.\n",
    "            for batch_idx in xrange(n_batches):\n",
    "                _, l = session.run([train_discriminator, discrim_cost], \\\n",
    "                                   feed_dict={X: train_X[batch_idx*batch_size:(batch_idx+1)*batch_size], \\\n",
    "                                              Y: train_Y[batch_idx*batch_size:(batch_idx+1)*batch_size]})\n",
    "                epoch_discrim_error += l\n",
    "            discrim_error += epoch_discrim_error/n_batches\n",
    "            \n",
    "        # print \"Epoch %d.  Discriminator error: %.3f\" %(epoch_idx, discrim_error)\n",
    "        \n",
    "        # train generator\n",
    "        for idx in xrange(n_generator_updates):\n",
    "            # sample from generator again\n",
    "            prior_samples = np.random.normal(size=(N, latent_d))\n",
    "        \n",
    "            # perform batch updates\n",
    "            for batch_idx in xrange(n_batches):\n",
    "                session.run(train_generator, feed_dict={Z: prior_samples[batch_idx*batch_size:(batch_idx+1)*batch_size], \\\n",
    "                                                        Y: Y_gen[batch_idx*batch_size:(batch_idx+1)*batch_size]})\n",
    "        \n",
    "        # visualize a sample to gauge progress\n",
    "        mnist_sample = session.run(generator_out_squashed, feed_dict={Z:np.random.normal(size=(1, latent_d))})\n",
    "        display.clear_output(wait=True)\n",
    "        plt.imshow(np.reshape(mnist_sample * 255., (28, 28)), cmap='Greys_r')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There has been recent work on trying to understand what GANs are doing in terms of classic estimation principles.  See [Shakir Mohamed's note](https://arxiv.org/abs/1610.03483) characterizing GANs as performing ratio tests, $p(x)/q(x)$ where $p(x)$ is the true distribution and $q(x)$ is the simulated one.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
