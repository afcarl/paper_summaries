{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pylab as pl\n",
    "\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Learning Generative Models with Sinkhorn Divergences](https://arxiv.org/abs/1706.00292)\n",
    "### by Aude Genevay, Gabriel Peyr√©, and Marco Cuturi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper proposes the *Sinkhorn divergence*, an optimal transport (OT) based optimization objective amenable to auto-differentiation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.  Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook, we'll use data drawn from a Gaussian Mixture Model (GMM).  Here is a function to draw samples from a GMM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw sample from mixture model\n",
    "# k ~ Mult(pi)\n",
    "# z ~ f_k\n",
    "def draw_samples(pi_arr, mu_arr, cov_arr, n_samples=100):\n",
    "    comp_arr = np.random.multinomial(n_samples, pi_arr)\n",
    "    z = []\n",
    "    y = []\n",
    "    for idx, count in enumerate(comp_arr):\n",
    "        for c in xrange(count):\n",
    "            y.append(idx)\n",
    "            z.append(np.random.multivariate_normal(mu_s[idx], cov_s[idx]))\n",
    "    return np.array(z), np.array(y)[np.newaxis].T\n",
    "\n",
    "def shuffle_in_unison_inplace(a, b):\n",
    "    assert a.shape[0] == b.shape[0]\n",
    "    p = np.random.permutation(a.shape[0])\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 300 # number of datapoints \n",
    "input_d = 2\n",
    "\n",
    "# Define mixture model\n",
    "pi = np.array([.35, .65])\n",
    "mu_s = [np.array([-5., -5.]), np.array([5., 5.])]\n",
    "cov_s = [np.array([[1., 0.], [0., 1.]]), np.array([[1., 0.], [0., 1.]])]\n",
    "\n",
    "# draw_samples\n",
    "X_train, y_train = draw_samples(pi, mu_s, cov_s, N)\n",
    "\n",
    "# shuffle \n",
    "X_train, y_train = shuffle_in_unison_inplace(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(X, Y):\n",
    "    return tf.reduce_sum((X - Y)**2, keep_dims=True)\n",
    "\n",
    "def sinkhorn_loss(X, Y, epsilon=1., L=3, n=10):\n",
    "    c = np.zeros((n, n)).tolist()\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            c[i][j] = cost(X[i, :], Y[j, :]) \n",
    "    \n",
    "    c = tf.concat(1, [tf.expand_dims(tf.concat(0, t), 1) for t in c])\n",
    "    K = tf.exp(-c/epsilon) \n",
    "    \n",
    "    a = tf.ones((n, 1))\n",
    "    b = tf.ones((n, 1))\n",
    "    for l in range(L):\n",
    "        a = 1./tf.matmul(K, b)\n",
    "        b = 1./tf.matmul(tf.transpose(K), a)\n",
    "    \n",
    "    return tf.matmul(tf.transpose(tf.matmul(tf.mul(K, c), b)), a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a neural network instead of a regression model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_neural_net(layer_sizes, std=.001):\n",
    "    # layer_sizes is a list of the input size, hidden layer sizes, and output size\n",
    "    params = {'w':[], 'b':[]}\n",
    "    for n_in, n_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "        params['w'].append(tf.Variable(tf.random_normal([n_in, n_out], stddev=std)))\n",
    "        params['b'].append(tf.Variable(tf.zeros([n_out,])))\n",
    "    return params\n",
    "\n",
    "def neural_net(X, params):\n",
    "    h = [X]\n",
    "    for w,b in zip(params['w'][:-1], params['b'][:-1]):\n",
    "        h.append( tf.nn.relu( tf.matmul(h[-1], w) + b ) )\n",
    "    # NOTE: no output activation.  TF will take care of this in pre-defined loss functions\n",
    "    return tf.matmul(h[-1], params['w'][-1]) + params['b'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the network's symbolic output and cost like we did for regression before..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Generative Adversarial Network (GAN)\n",
    "Now let's explore [Generative Adversarial Networks (GANs)](https://arxiv.org/abs/1406.2661) with TensorFlow.  GANs are composed of two neural networks.  One network is trying to classify simulated data from the real data.  The other network is trying to simulate data in such a way that the first net will be fooled.  The result of this process is that the second net gets better and better at simulating realistic data until eventually that data is indistinguishable from the real data.  The computational pipeline is summarized in the diagram below: \n",
    "![GAN_pipeline](./graphics/GAN.png)  GANs are refered to as 'implicit generative models' as there is an implied likelihood but not a well-specified one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the symbolic variables again.  This time we need another one, Z, that will be the samples drawn from the generator's latent space..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent_d = 50 # z ~ p(z), GAN prior\n",
    "hidden_d = 500 # num. of hidden units in NN\n",
    "\n",
    "### Make symbolic variables\n",
    "X = tf.placeholder(\"float\", [None, input_d]) # samples to discriminate\n",
    "Z = tf.placeholder(\"float\", [None, latent_d]) # samples from generator's latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the discriminator model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the generator model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator_params = init_neural_net([latent_d, hidden_d, input_d])\n",
    "generator_out = neural_net(Z, generator_params)\n",
    "\n",
    "loss = 2*sinkhorn_loss(X, generator_out) - sinkhorn_loss(X, X) - sinkhorn_loss(generator_out, generator_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSuT4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13cd9e110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n",
      "[[ nan]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def plot_densities(true_data, samples):\n",
    "    # clear the plot\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    plt.scatter(true_data[:,0], true_data[:,1], s=100, color='k', marker='x', label=\"data\")\n",
    "    plt.scatter(samples[:,0], samples[:,1], s=100, color='r', marker='o', label=\"samples\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.xlim([-15,15])\n",
    "    plt.ylim([-15,15])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Set training params\n",
    "n_epochs = 150\n",
    "learning_rate = 0.00 #1\n",
    "\n",
    "# create training ops\n",
    "train_generator = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=generator_params['w']+generator_params['b'])\n",
    "\n",
    "generator_weights = None\n",
    "generator_biases = None\n",
    "with tf.Session() as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for epoch_idx in xrange(n_epochs):\n",
    "        \n",
    "        loss_tracker = 0.\n",
    "        \n",
    "        # train generator\n",
    "        for idx in xrange(N/10):\n",
    "            # sample from generator again\n",
    "            prior_samples = np.random.normal(size=(10, latent_d))\n",
    "        \n",
    "            # perform update\n",
    "            _, l = session.run([train_generator, loss], feed_dict={Z: prior_samples, X: X_train[idx*10:(idx+1)*10, :]})\n",
    "            print l\n",
    "            print\n",
    "            loss_tracker += l\n",
    "        \n",
    "        # visualize progress\n",
    "        if epoch_idx%5 == 0: \n",
    "            plot_densities(X_train, session.run(generator_out, feed_dict={Z: np.random.normal(size=(N/2, latent_d))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GAN can capture one mode well but not both (at least for the settings I've tried).  This is known problem: \"A common problem with GAN framework is that the generator tends to only generate samples that are clustered in one or a few modes of the regions of high data density, instead of spanning the whole range\" [[source]](https://arxiv.org/pdf/1609.03126v2.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Training a GAN on MNIST\n",
    "Let's try to train a GAN on a subset of MNIST..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "# reduce dataset and normalize to [0,1]\n",
    "random_idxs = range(mnist.data.shape[0])\n",
    "shuffle(random_idxs)\n",
    "mnist_images = mnist.data[random_idxs[:5000],:] / 255.\n",
    "\n",
    "# show the first image\n",
    "plt.imshow(np.reshape(mnist_images[0,:] * 255., (28, 28)), cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same symbolic variables, discriminator, and generator as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, input_d = mnist_images.shape\n",
    "latent_d = 100 # z ~ p(z), GAN prior\n",
    "hidden_d_discrim = 1000 # num. of hidden units in discrim NN\n",
    "hidden_d_gen = 500 # num. of hidden units in gen NN\n",
    "\n",
    "### Make symbolic variables\n",
    "X = tf.placeholder(\"float\", [None, input_d]) # samples to discriminate\n",
    "Z = tf.placeholder(\"float\", [None, latent_d]) # samples to discriminate\n",
    "Y = tf.placeholder(\"float\", [None, 1]) # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_params = init_neural_net([input_d, hidden_d_discrim, hidden_d_discrim, 1]) \n",
    "discrim_out = neural_net(X, discrim_params)\n",
    "discrim_cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(discrim_out, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator_params = init_neural_net([latent_d, hidden_d_gen, input_d])\n",
    "generator_out = neural_net(Z, generator_params)\n",
    "\n",
    "# This line is new.  The images are on [0,1] so we need to apply a sigmoid to the samples.\n",
    "generator_out_squashed = tf.nn.sigmoid(generator_out)\n",
    "\n",
    "discrim_out_genUpdate = neural_net(generator_out_squashed, discrim_params)\n",
    "generator_cost = tf.reduce_mean(-tf.nn.sigmoid_cross_entropy_with_logits(discrim_out_genUpdate, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the GAN..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make labels for training\n",
    "X_true = mnist_images\n",
    "Y_true = np.ones((N,1))\n",
    "Y_gen = np.zeros((N,1))\n",
    "Y_concat = np.vstack([Y_true, Y_gen])\n",
    "\n",
    "# Set training params\n",
    "n_epochs = 25\n",
    "n_discrim_updates = 1\n",
    "n_generator_updates = 1\n",
    "d_learning_rate = .0002\n",
    "g_learning_rate = .00005\n",
    "batch_size = 120\n",
    "n_batches = N/batch_size\n",
    "\n",
    "# create training ops\n",
    "train_discriminator = tf.train.AdamOptimizer(d_learning_rate).minimize(discrim_cost, var_list=discrim_params['w']+discrim_params['b'])\n",
    "train_generator = tf.train.AdamOptimizer(g_learning_rate).minimize(generator_cost, var_list=generator_params['w']+generator_params['b'])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for epoch_idx in xrange(n_epochs):\n",
    "        \n",
    "        # train discriminator\n",
    "        discrim_error = 0.\n",
    "        for idx in xrange(n_discrim_updates):\n",
    "            # sample from generator\n",
    "            prior_samples = np.random.normal(size=(N, latent_d))\n",
    "            genSamples = session.run(generator_out, feed_dict={Z: prior_samples})\n",
    "        \n",
    "            # make dataset and shuffle\n",
    "            train_X = np.vstack([X_true, genSamples])\n",
    "            train_X, train_Y = shuffle_in_unison_inplace(train_X, Y_concat)\n",
    "        \n",
    "            # perform batch updates\n",
    "            epoch_discrim_error = 0.\n",
    "            for batch_idx in xrange(n_batches):\n",
    "                _, l = session.run([train_discriminator, discrim_cost], \\\n",
    "                                   feed_dict={X: train_X[batch_idx*batch_size:(batch_idx+1)*batch_size], \\\n",
    "                                              Y: train_Y[batch_idx*batch_size:(batch_idx+1)*batch_size]})\n",
    "                epoch_discrim_error += l\n",
    "            discrim_error += epoch_discrim_error/n_batches\n",
    "            \n",
    "        # print \"Epoch %d.  Discriminator error: %.3f\" %(epoch_idx, discrim_error)\n",
    "        \n",
    "        # train generator\n",
    "        for idx in xrange(n_generator_updates):\n",
    "            # sample from generator again\n",
    "            prior_samples = np.random.normal(size=(N, latent_d))\n",
    "        \n",
    "            # perform batch updates\n",
    "            for batch_idx in xrange(n_batches):\n",
    "                session.run(train_generator, feed_dict={Z: prior_samples[batch_idx*batch_size:(batch_idx+1)*batch_size], \\\n",
    "                                                        Y: Y_gen[batch_idx*batch_size:(batch_idx+1)*batch_size]})\n",
    "        \n",
    "        # visualize a sample to gauge progress\n",
    "        mnist_sample = session.run(generator_out_squashed, feed_dict={Z:np.random.normal(size=(1, latent_d))})\n",
    "        display.clear_output(wait=True)\n",
    "        plt.imshow(np.reshape(mnist_sample * 255., (28, 28)), cmap='Greys_r')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There has been recent work on trying to understand what GANs are doing in terms of classic estimation principles.  See [Shakir Mohamed's note](https://arxiv.org/abs/1610.03483) characterizing GANs as performing ratio tests, $p(x)/q(x)$ where $p(x)$ is the true distribution and $q(x)$ is the simulated one.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
